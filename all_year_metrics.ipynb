{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4269855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78424217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG: season=MONSOON, window months=6-7\n"
     ]
    }
   ],
   "source": [
    "season = \"MONSOON\"\n",
    "model_folder = \"random_forest\" \n",
    "model_name = \"Random_Forest_t0.410\"\n",
    "\n",
    "# Configure season window and output directories\n",
    "if season.upper() == \"PREMONSOON\":\n",
    "    start_month, end_month = 3,5   # Mar–May\n",
    "else:\n",
    "    start_month, end_month = 6, 7  # Jun–Oct (adjust if needed)\n",
    "\n",
    "CONFIG = {\n",
    "    \"season\": season,\n",
    "    \"start_month\": start_month,\n",
    "    \"end_month\": end_month,\n",
    "    # Observations file (relative to workspace, matching project structure)\n",
    "    \"obs_excel\": Path(\"..\") / \"LSTM\" / \"PREMONSOON-janjul201724.xlsx\",\n",
    "    # Base dir where prediction CSVs and outputs live\n",
    "    \"base_dir\": Path(\"predictionsv2\") / model_folder / model_name / season,\n",
    "    # Explicit predictions dir alias for convenience\n",
    "    \"predictions_dir\":  Path(\"predictionsv2\") / model_folder / model_name / season,\n",
    "}\n",
    "\n",
    "print(f\"CONFIG: season={CONFIG['season']}, window months={CONFIG['start_month']}-{CONFIG['end_month']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b9d667d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded event flags data: 244 rows, columns=['1-day', '2-day', '3-day', '4-day', '5-day', '6-day', '7-day', '8-day', '9-day', '10-day']\n"
     ]
    }
   ],
   "source": [
    "# Load event flags across years and build a single dataframe indexed by Date\n",
    "years = [2017, 2022, 2023, 2024]\n",
    "base_dir = CONFIG['base_dir']\n",
    "\n",
    "frames = []\n",
    "for y in years:\n",
    "    csv_path = base_dir / f\"sum_f_{y}.csv\"\n",
    "    df_y = pd.read_csv(csv_path)\n",
    "\n",
    "    # Normalize date column\n",
    "    date_col = None\n",
    "    for cand in [\"Date\", \"date\", \"DATE\", \"Date Time\", \"datetime\"]:\n",
    "        if cand in df_y.columns:\n",
    "            date_col = cand\n",
    "            break\n",
    "    if date_col is None:\n",
    "        # If first column looks like date, use it\n",
    "        first_col = df_y.columns[0]\n",
    "        try:\n",
    "            pd.to_datetime(df_y[first_col])\n",
    "            date_col = first_col\n",
    "        except Exception:\n",
    "            raise ValueError(f\"No date-like column found in {csv_path}\")\n",
    "\n",
    "    df_y[\"Date\"] = pd.to_datetime(df_y[date_col]).dt.normalize()\n",
    "    df_y = df_y.drop(columns=[c for c in [date_col] if c != \"Date\" and c in df_y.columns])\n",
    "    frames.append(df_y)\n",
    "\n",
    "sum_f = pd.concat(frames, ignore_index=True)\n",
    "sum_f = sum_f.sort_values(\"Date\").reset_index(drop=True)\n",
    "sum_f_indexed = sum_f.set_index(\"Date\")\n",
    "\n",
    "print(f\"Loaded event flags data: {len(sum_f_indexed)} rows, columns={list(sum_f_indexed.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4a1cdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "EVENT-BASED EVALUATION METRICS (All Years) BY LEAD DAY + OVERALL\n",
      "====================================================================================================\n",
      "       Lead Day  TP (Hits)  FN (Misses)  FP (False Alarms)  TN (Correct Negatives)  Precision   Recall  F1 Score      FAR\n",
      "          1-day          6            5                 29                     204   0.171429 0.545455  0.260870 0.828571\n",
      "          2-day          5            6                 41                     192   0.108696 0.454545  0.175439 0.891304\n",
      "          3-day          5            6                 48                     185   0.094340 0.454545  0.156250 0.905660\n",
      "          4-day          8            3                 46                     187   0.148148 0.727273  0.246154 0.851852\n",
      "          5-day          6            5                 45                     188   0.117647 0.545455  0.193548 0.882353\n",
      "          6-day          2            9                 42                     191   0.045455 0.181818  0.072727 0.954545\n",
      "          7-day          3            8                 44                     189   0.063830 0.272727  0.103448 0.936170\n",
      "          8-day          5            6                 49                     184   0.092593 0.454545  0.153846 0.907407\n",
      "          9-day          5            6                 43                     190   0.104167 0.454545  0.169492 0.895833\n",
      "         10-day          5            6                 39                     194   0.113636 0.454545  0.181818 0.886364\n",
      "Overall (union)         11            0                150                      83   0.068323 1.000000  0.127907 0.931677\n",
      "\n",
      "Saved evaluation metrics to: predictionsv2\\random_forest\\Random_Forest_t0.410\\MONSOON\\evaluation_metrics_all_years.csv\n",
      "\n",
      "====================================================================================================\n",
      "PREDICTED DATES BY LEAD DAY vs ACTUAL FLOOD DATES (All Years)\n",
      "====================================================================================================\n",
      "        1-day      2-day      3-day      4-day      5-day      6-day      7-day      8-day      9-day     10-day Actual Flood Date\n",
      "0  2017-06-03 2017-06-04 2017-06-04 2017-06-05 2017-06-15 2017-06-07 2017-06-13 2017-06-13 2017-06-20 2017-06-21        2017-06-17\n",
      "1  2017-06-04 2017-06-05 2017-06-05 2017-06-06 2017-06-16 2017-06-15 2017-06-16 2017-06-16 2017-06-23 2017-06-26        2022-06-15\n",
      "2  2017-06-05 2017-06-13 2017-06-06 2017-06-14 2017-06-18 2017-06-18 2017-06-19 2017-06-17 2017-06-25 2017-06-27        2022-06-16\n",
      "3  2017-06-14 2017-06-14 2017-06-07 2017-06-15 2017-06-21 2017-06-19 2017-06-20 2017-06-19 2017-07-04 2017-06-28        2022-06-17\n",
      "4  2017-06-19 2017-06-15 2017-06-14 2017-06-16 2017-06-22 2017-06-22 2017-06-21 2017-06-20 2017-07-05 2017-07-02        2022-06-18\n",
      "5  2017-06-21 2017-06-17 2017-06-15 2017-06-17 2017-07-03 2017-06-26 2017-06-23 2017-06-22 2017-07-06 2017-07-04        2022-06-19\n",
      "6  2017-07-06 2017-06-18 2017-06-19 2017-06-20 2017-07-04 2017-06-29 2017-06-29 2017-06-24 2017-07-07 2017-07-05        2024-06-16\n",
      "7  2017-07-18 2017-06-21 2017-06-20 2017-06-21 2017-07-05 2017-07-04 2017-07-04 2017-06-26 2017-07-08 2017-07-07        2024-06-17\n",
      "8  2017-07-21 2017-06-22 2017-06-22 2017-06-23 2017-07-07 2017-07-05 2017-07-05 2017-07-05 2017-07-09 2017-07-11        2024-06-18\n",
      "9  2017-07-29 2017-06-29 2017-06-24 2017-07-03 2017-07-08 2017-07-06 2017-07-06 2017-07-06 2017-07-10 2017-07-15        2024-07-01\n",
      "10 2022-06-17 2017-07-03 2017-07-03 2017-07-09 2017-07-11 2017-07-07 2017-07-08 2017-07-07 2017-07-17 2017-07-16        2024-07-02\n",
      "11 2022-06-19 2017-07-05 2017-07-04 2017-07-10 2017-07-12 2017-07-08 2017-07-09 2017-07-08 2022-06-13 2017-07-17               NaT\n",
      "12 2022-06-20 2017-07-07 2017-07-05 2017-07-11 2017-07-13 2017-07-09 2017-07-13 2017-07-09 2022-06-14 2022-06-14               NaT\n",
      "13 2022-06-22 2017-07-08 2017-07-06 2017-07-12 2017-07-14 2017-07-11 2017-07-14 2017-07-10 2022-06-16 2022-06-15               NaT\n",
      "14 2023-06-02 2017-07-13 2017-07-13 2017-07-13 2017-07-15 2017-07-12 2017-07-15 2017-07-12 2022-06-17 2022-06-16               NaT\n",
      "15 2023-06-14 2017-07-21 2017-07-21 2017-07-14 2017-07-16 2017-07-14 2017-07-21 2017-07-14 2022-06-18 2022-06-17               NaT\n",
      "16 2023-06-17 2017-07-26 2022-06-16 2017-07-17 2017-07-21 2017-07-30 2022-06-10 2017-07-16 2022-06-21 2022-06-20               NaT\n",
      "17 2023-06-18 2017-07-29 2022-06-17 2017-07-20 2022-06-15 2017-07-31 2022-06-15 2022-06-13 2022-06-22 2022-07-04               NaT\n",
      "18 2023-06-22 2022-06-18 2022-06-18 2022-06-16 2022-06-16 2022-06-17 2022-06-17 2022-06-16 2022-07-04 2022-07-08               NaT\n",
      "19 2023-06-25 2022-06-19 2022-06-19 2022-06-19 2022-06-17 2022-06-21 2022-06-22 2022-06-17 2022-07-07 2022-07-09               NaT\n",
      "20 2023-07-29 2022-06-20 2022-06-21 2022-06-20 2022-06-20 2022-06-22 2022-07-08 2022-06-18 2022-07-22 2022-07-22               NaT\n",
      "21 2024-06-16 2022-06-21 2022-06-27 2022-06-22 2022-06-21 2022-07-03 2022-07-10 2022-06-20 2022-07-23 2022-07-27               NaT\n",
      "22 2024-06-17 2022-06-27 2022-07-12 2022-06-29 2022-06-22 2023-06-12 2022-07-17 2022-06-22 2022-07-28 2023-06-16               NaT\n",
      "23 2024-06-18 2022-06-30 2022-07-13 2022-07-14 2022-06-25 2023-06-16 2022-07-21 2022-07-03 2023-06-12 2023-06-18               NaT\n",
      "24 2024-06-19 2022-07-13 2022-07-16 2022-07-18 2022-07-20 2023-06-18 2023-06-13 2022-07-07 2023-06-17 2023-06-19               NaT\n",
      "25 2024-06-20 2022-07-30 2022-07-18 2022-07-19 2023-06-08 2023-06-19 2023-06-20 2022-07-16 2023-06-18 2023-06-20               NaT\n",
      "26 2024-06-21 2023-06-06 2022-07-25 2022-07-22 2023-06-12 2023-06-21 2023-06-22 2022-07-18 2023-06-19 2023-06-22               NaT\n",
      "27 2024-06-22 2023-06-08 2022-07-30 2023-06-12 2023-06-13 2023-06-22 2023-06-23 2022-07-21 2023-06-24 2023-06-25               NaT\n",
      "28 2024-07-02 2023-06-16 2023-06-17 2023-06-17 2023-06-17 2023-06-25 2023-06-28 2022-07-29 2023-07-05 2023-06-28               NaT\n",
      "29 2024-07-03 2023-06-17 2023-06-18 2023-06-19 2023-06-18 2023-06-29 2023-07-03 2023-06-18 2023-07-08 2023-07-04               NaT\n",
      "30 2024-07-04 2023-06-25 2023-06-19 2023-06-20 2023-06-19 2023-07-03 2023-07-04 2023-06-23 2023-07-14 2023-07-09               NaT\n",
      "31 2024-07-06 2023-07-18 2023-06-20 2023-06-21 2023-06-20 2023-07-05 2023-07-17 2023-06-28 2023-07-18 2023-07-13               NaT\n",
      "32 2024-07-14 2023-07-28 2023-06-21 2023-06-22 2023-06-21 2024-06-12 2023-07-19 2023-06-29 2023-07-21 2023-07-19               NaT\n",
      "33 2024-07-17 2024-06-03 2023-07-01 2023-07-03 2023-06-23 2024-06-18 2023-07-22 2023-07-04 2023-07-22 2024-06-16               NaT\n",
      "34 2024-07-19 2024-06-10 2023-07-03 2023-07-04 2023-07-22 2024-06-20 2024-06-12 2023-07-09 2024-06-12 2024-06-19               NaT\n",
      "35        NaT 2024-06-12 2023-07-04 2023-07-05 2024-06-09 2024-06-22 2024-06-21 2023-07-21 2024-06-16 2024-06-20               NaT\n",
      "36        NaT 2024-06-14 2023-07-15 2023-07-19 2024-06-14 2024-07-03 2024-06-22 2024-06-11 2024-06-23 2024-07-02               NaT\n",
      "37        NaT 2024-06-18 2023-07-23 2023-07-20 2024-06-17 2024-07-05 2024-06-23 2024-06-13 2024-06-27 2024-07-05               NaT\n",
      "38        NaT 2024-06-19 2024-06-15 2023-07-23 2024-06-19 2024-07-06 2024-06-24 2024-06-21 2024-06-29 2024-07-06               NaT\n",
      "39        NaT 2024-06-21 2024-06-19 2024-06-16 2024-06-22 2024-07-07 2024-06-30 2024-06-22 2024-07-02 2024-07-08               NaT\n",
      "40        NaT 2024-07-02 2024-06-20 2024-06-17 2024-07-01 2024-07-12 2024-07-02 2024-06-23 2024-07-03 2024-07-14               NaT\n",
      "41        NaT 2024-07-04 2024-06-21 2024-06-18 2024-07-02 2024-07-13 2024-07-03 2024-06-24 2024-07-05 2024-07-16               NaT\n",
      "42        NaT 2024-07-07 2024-06-22 2024-06-22 2024-07-03 2024-07-19 2024-07-05 2024-07-02 2024-07-06 2024-07-20               NaT\n",
      "43        NaT 2024-07-13 2024-06-29 2024-06-23 2024-07-04 2024-07-21 2024-07-06 2024-07-03 2024-07-07 2024-07-30               NaT\n",
      "44        NaT 2024-07-14 2024-06-30 2024-06-25 2024-07-05        NaT 2024-07-12 2024-07-04 2024-07-08        NaT               NaT\n",
      "45        NaT 2024-07-19 2024-07-02 2024-07-01 2024-07-06        NaT 2024-07-14 2024-07-06 2024-07-13        NaT               NaT\n",
      "46        NaT        NaT 2024-07-05 2024-07-02 2024-07-07        NaT 2024-07-15 2024-07-07 2024-07-15        NaT               NaT\n",
      "47        NaT        NaT 2024-07-07 2024-07-03 2024-07-11        NaT        NaT 2024-07-08 2024-07-19        NaT               NaT\n",
      "48        NaT        NaT 2024-07-12 2024-07-04 2024-07-12        NaT        NaT 2024-07-09        NaT        NaT               NaT\n",
      "49        NaT        NaT 2024-07-13 2024-07-05 2024-07-15        NaT        NaT 2024-07-13        NaT        NaT               NaT\n",
      "50        NaT        NaT 2024-07-14 2024-07-12 2024-07-28        NaT        NaT 2024-07-14        NaT        NaT               NaT\n",
      "51        NaT        NaT 2024-07-19 2024-07-13        NaT        NaT        NaT 2024-07-15        NaT        NaT               NaT\n",
      "52        NaT        NaT 2024-07-29 2024-07-16        NaT        NaT        NaT 2024-07-20        NaT        NaT               NaT\n",
      "53        NaT        NaT        NaT 2024-07-28        NaT        NaT        NaT 2024-07-21        NaT        NaT               NaT\n",
      "\n",
      "Saved event dates to: predictionsv2\\random_forest\\Random_Forest_t0.410\\MONSOON\\event_dates_by_lead_all_years.csv\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# EVENT-BASED EVALUATION METRICS (All Years)\n",
    "# ====================================\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "df_obs = pd.read_excel(CONFIG['obs_excel'])\n",
    "df_obs['Date'] = pd.to_datetime(df_obs['Date Time']).dt.normalize()\n",
    "\n",
    "# Calculate daily water levels and set threshold\n",
    "wl_daily = df_obs.groupby('Date')['WL (mMSL)'].mean().reset_index()\n",
    "threshold = 12.34 if CONFIG['season'].upper() == 'MONSOON' else 10.69\n",
    "wl_daily['label'] = (wl_daily['WL (mMSL)'] >= threshold).astype(int)\n",
    "\n",
    "# Allowed months for the configured season\n",
    "allowed_months = list(range(CONFIG['start_month'], CONFIG['end_month'] + 1))\n",
    "\n",
    "# Actual flood dates across all years within allowed months\n",
    "flood_in_season = df_obs[(df_obs['WL (mMSL)'] >= threshold) &\n",
    "                        (df_obs['Date'].dt.month.isin(allowed_months))]\n",
    "actual_flood_dates = set(flood_in_season['Date'].dt.date.unique())\n",
    "\n",
    "# Evaluation dates: all dates present in predictions within allowed months\n",
    "all_dates = sorted([d for d in sum_f_indexed.index if d.month in allowed_months])\n",
    "\n",
    "if len(all_dates) == 0:\n",
    "    raise ValueError(\"No dates found in sum_f_indexed for the configured season months.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Lead-day wise metrics\n",
    "# -----------------------------\n",
    "results = []\n",
    "\n",
    "for lead_col in sum_f_indexed.columns:\n",
    "    # Skip non-lead columns if any\n",
    "    if not isinstance(lead_col, str) or '-' not in lead_col:\n",
    "        continue\n",
    "\n",
    "    days = int(lead_col.split('-')[0])\n",
    "\n",
    "    # Event flags: any non-zero means event (1), else 0\n",
    "    flagged_idx = sum_f_indexed.index[sum_f_indexed[lead_col] != 0]\n",
    "\n",
    "    # Verification dates (init_date + lead_days) within season months\n",
    "    time_event = (flagged_idx + pd.Timedelta(days=days)).normalize()\n",
    "    predicted_event_dates = {d.date() for d in time_event if d.month in allowed_months}\n",
    "\n",
    "    y_true = np.array([d.date() in actual_flood_dates for d in all_dates]).astype(int)\n",
    "    y_pred = np.array([d.date() in predicted_event_dates for d in all_dates]).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
    "    TP, FN = cm[0, 0], cm[0, 1]\n",
    "    FP, TN = cm[1, 0], cm[1, 1]\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else float('nan')\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else float('nan')\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 and not np.isnan(precision) and not np.isnan(recall) else float('nan')\n",
    "    far = FP / (TP + FP) if (TP + FP) > 0 else float('nan')\n",
    "\n",
    "    results.append({\n",
    "        'Lead Day': lead_col,\n",
    "        'TP (Hits)': TP,\n",
    "        'FN (Misses)': FN,\n",
    "        'FP (False Alarms)': FP,\n",
    "        'TN (Correct Negatives)': TN,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'FAR': far,\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(results)\n",
    "\n",
    "# -----------------------------\n",
    "# Overall metrics (union across all leads)\n",
    "# -----------------------------\n",
    "union_predicted = set()\n",
    "for lead_col in sum_f_indexed.columns:\n",
    "    if not isinstance(lead_col, str) or '-' not in lead_col:\n",
    "        continue\n",
    "    days = int(lead_col.split('-')[0])\n",
    "    flagged_idx = sum_f_indexed.index[sum_f_indexed[lead_col] != 0]\n",
    "    time_event = (flagged_idx + pd.Timedelta(days=days)).normalize()\n",
    "    union_predicted.update({d.date() for d in time_event if d.month in allowed_months})\n",
    "\n",
    "y_true_overall = np.array([d.date() in actual_flood_dates for d in all_dates]).astype(int)\n",
    "y_pred_overall = np.array([d.date() in union_predicted for d in all_dates]).astype(int)\n",
    "\n",
    "cm_o = confusion_matrix(y_true_overall, y_pred_overall, labels=[1, 0])\n",
    "TP_o, FN_o = cm_o[0, 0], cm_o[0, 1]\n",
    "FP_o, TN_o = cm_o[1, 0], cm_o[1, 1]\n",
    "\n",
    "precision_o = TP_o / (TP_o + FP_o) if (TP_o + FP_o) > 0 else float('nan')\n",
    "recall_o = TP_o / (TP_o + FN_o) if (TP_o + FN_o) > 0 else float('nan')\n",
    "f1_o = 2 * precision_o * recall_o / (precision_o + recall_o) if (precision_o + recall_o) > 0 and not np.isnan(precision_o) and not np.isnan(recall_o) else float('nan')\n",
    "far_o = FP_o / (TP_o + FP_o) if (TP_o + FP_o) > 0 else float('nan')\n",
    "\n",
    "overall_row = pd.DataFrame([{ \n",
    "    'Lead Day': 'Overall (union)',\n",
    "    'TP (Hits)': TP_o,\n",
    "    'FN (Misses)': FN_o,\n",
    "    'FP (False Alarms)': FP_o,\n",
    "    'TN (Correct Negatives)': TN_o,\n",
    "    'Precision': precision_o,\n",
    "    'Recall': recall_o,\n",
    "    'F1 Score': f1_o,\n",
    "    'FAR': far_o,\n",
    "}])\n",
    "\n",
    "summary_with_overall = pd.concat([summary_df, overall_row], ignore_index=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"EVENT-BASED EVALUATION METRICS (All Years) BY LEAD DAY + OVERALL\")\n",
    "print(\"=\"*100)\n",
    "print(summary_with_overall.to_string(index=False))\n",
    "\n",
    "# Save summary to CSV (same directory as predictions)\n",
    "out_summary = CONFIG['base_dir'] / 'evaluation_metrics_all_years.csv'\n",
    "summary_with_overall.to_csv(out_summary, index=False)\n",
    "print(f\"\\nSaved evaluation metrics to: {out_summary}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Detailed dates by lead (padded)\n",
    "# -----------------------------\n",
    "dates_event = pd.DataFrame()\n",
    "\n",
    "# Determine max length across lead columns (after filtering to season months)\n",
    "max_len = 0\n",
    "for lead_col in sum_f_indexed.columns:\n",
    "    if not isinstance(lead_col, str) or '-' not in lead_col:\n",
    "        continue\n",
    "    days = int(lead_col.split('-')[0])\n",
    "    flagged_idx = sum_f_indexed.index[sum_f_indexed[lead_col] != 0]\n",
    "    time_event = (flagged_idx + pd.Timedelta(days=days)).normalize()\n",
    "    time_event = pd.DatetimeIndex([d for d in time_event if d.month in allowed_months])\n",
    "    max_len = max(max_len, len(time_event))\n",
    "\n",
    "# Build padded columns\n",
    "for lead_col in sum_f_indexed.columns:\n",
    "    if not isinstance(lead_col, str) or '-' not in lead_col:\n",
    "        continue\n",
    "    days = int(lead_col.split('-')[0])\n",
    "    flagged_idx = sum_f_indexed.index[sum_f_indexed[lead_col] != 0]\n",
    "    time_event = (flagged_idx + pd.Timedelta(days=days)).normalize()\n",
    "    time_event = pd.DatetimeIndex([d for d in time_event if d.month in allowed_months])\n",
    "\n",
    "    padded = pd.Series(time_event.values, index=range(len(time_event))).reindex(range(max_len))\n",
    "    dates_event[lead_col] = padded\n",
    "\n",
    "# Add actual flood dates column (season months only)\n",
    "actual_flood_dates_sorted = sorted(pd.to_datetime(list(actual_flood_dates)))\n",
    "dates_event['Actual Flood Date'] = pd.Series(actual_flood_dates_sorted, index=range(len(actual_flood_dates_sorted))).reindex(range(max_len))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"PREDICTED DATES BY LEAD DAY vs ACTUAL FLOOD DATES (All Years)\")\n",
    "print(\"=\"*100)\n",
    "print(dates_event.to_string())\n",
    "\n",
    "# Save detailed dates\n",
    "out_dates = CONFIG['base_dir'] / 'event_dates_by_lead_all_years.csv'\n",
    "dates_event.to_csv(out_dates, index=False)\n",
    "print(f\"\\nSaved event dates to: {out_dates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0254b91b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "feaa4ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Combined output folder: D:\\Masters\\data\\logistic regression\\predictions\\combined\n",
      "\n",
      "\n",
      "===== MONSOON =====\n",
      "Saving to: D:\\Masters\\data\\logistic regression\\predictions\\combined\\MONSOON\n",
      "  → Loaded 6 flood spells for spell-based evaluation\n",
      "  ✓ Logistic_Regression: saved overall & per-lead (spell-based metrics)\n",
      "  → Loaded 6 flood spells for spell-based evaluation\n",
      "  ✓ Random_Forest: saved overall & per-lead (spell-based metrics)\n",
      "\n",
      "Season comparison (SPELL-BASED METRICS):\n",
      "Hit window = [Spell_Start-2d, Spell_End]: predictions here = TP\n",
      "Actual flood = [Spell_Start, Spell_End]: NO predictions here = FN\n",
      "Early warning = [Spell_Start-2d, Spell_Start-1]: NOT counted as miss if empty\n",
      "              Model  TP  FN  FP  TN  Precision  Recall    F1   FAR\n",
      "Logistic_Regression  13   3  46 178      0.220   0.812 0.347 0.780\n",
      "      Random_Forest  20   0 154  70      0.115   1.000 0.206 0.885\n",
      "Saved to: D:\\Masters\\data\\logistic regression\\predictions\\combined\\MONSOON\\overall_comparison.csv\n",
      "\n",
      "===== PREMONSOON =====\n",
      "Saving to: D:\\Masters\\data\\logistic regression\\predictions\\combined\\PREMONSOON\n",
      "  → Loaded 7 flood spells for spell-based evaluation\n",
      "  ✓ Logistic_Regression: saved overall & per-lead (spell-based metrics)\n",
      "  → Loaded 7 flood spells for spell-based evaluation\n",
      "  ✓ Random_Forest: saved overall & per-lead (spell-based metrics)\n",
      "\n",
      "Season comparison (SPELL-BASED METRICS):\n",
      "Hit window = [Spell_Start-2d, Spell_End]: predictions here = TP\n",
      "Actual flood = [Spell_Start, Spell_End]: NO predictions here = FN\n",
      "Early warning = [Spell_Start-2d, Spell_Start-1]: NOT counted as miss if empty\n",
      "              Model  TP  FN  FP  TN  Precision  Recall    F1   FAR\n",
      "Logistic_Regression  19  12  14 311      0.576   0.613 0.594 0.424\n",
      "      Random_Forest  35   1  63 262      0.357   0.972 0.522 0.643\n",
      "Saved to: D:\\Masters\\data\\logistic regression\\predictions\\combined\\PREMONSOON\\overall_comparison.csv\n",
      "\n",
      "✓✓✓ All outputs in combined folder: D:\\Masters\\data\\logistic regression\\predictions\\combined\n"
     ]
    }
   ],
   "source": [
    "# Season-wise overall metrics (union-of-leads) for both models\n",
    "# USING SPELL-BASED EVALUATION LOGIC\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Model directories\n",
    "model_dirs = {\n",
    "    \"MONSOON\": {\n",
    "        \"Logistic_Regression\": Path(r\"D:\\Masters\\data\\logistic regression\\predictions\\logistic_regression\\Logistic_Regression_t0.891\\MONSOON\"),\n",
    "        \"Random_Forest\": Path(r\"D:\\Masters\\data\\logistic regression\\predictions\\random_forest\\Random_Forest_t0.410\\MONSOON\"),\n",
    "    },\n",
    "    \"PREMONSOON\": {\n",
    "        \"Logistic_Regression\": Path(r\"D:\\Masters\\data\\logistic regression\\predictions\\logistic_regression\\Logistic_Regression_t0.887\\PREMONSOON\"),\n",
    "        \"Random_Forest\": Path(r\"D:\\Masters\\data\\logistic regression\\predictions\\random_forest\\Random_Forest_t0.400\\PREMONSOON\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "years = [2017, 2022, 2023, 2024]\n",
    "obs_excel = Path(\"..\") / \"LSTM\" / \"PREMONSOON-janjul201724.xlsx\"\n",
    "\n",
    "# OUTPUT: Save to combined folder\n",
    "combined_root = Path(r\"D:\\Masters\\data\\logistic regression\\predictions\\combined\")\n",
    "combined_root.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"✓ Combined output folder: {combined_root}\\n\")\n",
    "\n",
    "def season_months(season: str):\n",
    "    s = season.upper()\n",
    "    if s == \"PREMONSOON\":\n",
    "        return 3, 5\n",
    "    return 6, 7\n",
    "\n",
    "def build_all_dates(years, start_month, end_month):\n",
    "    alld = []\n",
    "    for y in years:\n",
    "        start = pd.Timestamp(y, start_month, 1)\n",
    "        end = (pd.Timestamp(y, end_month, 1) + pd.offsets.MonthEnd(1)).normalize()\n",
    "        rng = pd.date_range(start, end, freq='D')\n",
    "        alld.extend(rng)\n",
    "    return pd.DatetimeIndex(sorted(pd.to_datetime(alld).unique()))\n",
    "\n",
    "def load_flood_spells(model_dir: Path, years: list[int]):\n",
    "    \"\"\"Load flood spell periods from flood_time_periods_{year}.csv files\"\"\"\n",
    "    all_spells = []\n",
    "    for y in years:\n",
    "        fp = model_dir / f\"flood_time_periods_{y}.csv\"\n",
    "        if not fp.exists():\n",
    "            continue\n",
    "        df = pd.read_csv(fp)\n",
    "        df['Year'] = y\n",
    "        all_spells.append(df)\n",
    "    \n",
    "    if not all_spells:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    return pd.concat(all_spells, ignore_index=True)\n",
    "\n",
    "def build_spell_windows(spells_df):\n",
    "    \"\"\"Build spell window dictionaries from flood spells dataframe\"\"\"\n",
    "    spell_windows = {}\n",
    "    day_to_spells = {}\n",
    "    \n",
    "    for idx, row in spells_df.iterrows():\n",
    "        spell_id = f\"{row['Year']}_S{int(row['Spell_Number'])}\"\n",
    "        spell_start = pd.to_datetime(row['Spell_Start']).normalize().date()\n",
    "        spell_end = pd.to_datetime(row['Spell_End']).normalize().date()\n",
    "        \n",
    "        # Hit window: [Spell_Start - 2 days, Spell_End]\n",
    "        hit_window_start = (pd.to_datetime(spell_start) - pd.Timedelta(days=2)).date()\n",
    "        hit_window_end = spell_end\n",
    "        \n",
    "        spell_windows[spell_id] = {\n",
    "            'spell_start': spell_start,\n",
    "            'spell_end': spell_end,\n",
    "            'actual_flood_start': spell_start,\n",
    "            'actual_flood_end': spell_end,\n",
    "            'hit_window_start': hit_window_start,\n",
    "            'hit_window_end': hit_window_end,\n",
    "            'tol_window_start': hit_window_start,\n",
    "            'tol_window_end': spell_end,\n",
    "        }\n",
    "        \n",
    "        # Map each day to spell info\n",
    "        for d in pd.date_range(hit_window_start, hit_window_end, freq='D'):\n",
    "            if d.date() not in day_to_spells:\n",
    "                day_to_spells[d.date()] = []\n",
    "            day_to_spells[d.date()].append({\n",
    "                'spell_id': spell_id,\n",
    "                'in_hit_window': hit_window_start <= d.date() <= hit_window_end,\n",
    "                'in_actual_flood': spell_start <= d.date() <= spell_end\n",
    "            })\n",
    "    \n",
    "    return spell_windows, day_to_spells\n",
    "\n",
    "def extract_pred_from_sumf(model_dir: Path, years: list[int], allowed_months: list[int], all_dates):\n",
    "    pred_by_lead = {}\n",
    "    for y in years:\n",
    "        fp = model_dir / f\"sum_f_{y}.csv\"\n",
    "        if not fp.exists():\n",
    "            continue\n",
    "        df = pd.read_csv(fp, index_col=0)\n",
    "        idx = pd.to_datetime(df.index, errors='coerce')\n",
    "        df.index = idx\n",
    "        df = df[~df.index.isna()]\n",
    "        lead_cols = [c for c in df.columns if isinstance(c, str) and \"-day\" in c]\n",
    "        for c in lead_cols:\n",
    "            try:\n",
    "                days = int(str(c).split('-')[0])\n",
    "            except:\n",
    "                continue\n",
    "            flagged_idx = df.index[df[c].fillna(0) != 0]\n",
    "            verif_dates = (flagged_idx + pd.Timedelta(days=days)).normalize()\n",
    "            verif_dates = verif_dates[verif_dates.month.isin(allowed_months)]\n",
    "            verif_dates = verif_dates[verif_dates.isin(all_dates)]\n",
    "            s = set(pd.to_datetime(verif_dates).date)\n",
    "            if c not in pred_by_lead:\n",
    "                pred_by_lead[c] = s\n",
    "            else:\n",
    "                pred_by_lead[c] |= s\n",
    "    return pred_by_lead\n",
    "\n",
    "def compute_confusion_spell_based(all_dates, spell_windows, day_to_spells, pred_set):\n",
    "    \"\"\"\n",
    "    Compute confusion matrix using spell-based logic:\n",
    "    - Hit window = [Spell_Start-2d, Spell_End]: predictions here = TP\n",
    "    - Actual flood = [Spell_Start, Spell_End]: NO predictions here = FN (miss)\n",
    "    - Early warning = [Spell_Start-2d, Spell_Start-1]: NOT counted as miss if empty\n",
    "    - FP = predictions outside ALL tolerance windows\n",
    "    \"\"\"\n",
    "    TP, FN, FP, TN = 0, 0, 0, 0\n",
    "    \n",
    "    # Build set of all dates in any tolerance window\n",
    "    all_tolerance_dates = set()\n",
    "    for window in spell_windows.values():\n",
    "        for d in pd.date_range(window['tol_window_start'], window['tol_window_end'], freq='D'):\n",
    "            all_tolerance_dates.add(d.date())\n",
    "    \n",
    "    for d in all_dates:\n",
    "        d_date = d.date()\n",
    "        is_predicted = d_date in pred_set\n",
    "        \n",
    "        # Check if this day is in any spell's window\n",
    "        spell_info = day_to_spells.get(d_date, [])\n",
    "        \n",
    "        if spell_info:\n",
    "            # This day is in hit window of at least one spell\n",
    "            in_hit_window = any(s['in_hit_window'] for s in spell_info)\n",
    "            in_actual_flood = any(s['in_actual_flood'] for s in spell_info)\n",
    "            \n",
    "            if in_hit_window:\n",
    "                if is_predicted:\n",
    "                    TP += 1\n",
    "                elif in_actual_flood:\n",
    "                    # Miss: in ACTUAL flood period with NO prediction\n",
    "                    FN += 1\n",
    "                # else: in early warning window with no prediction = NOT a miss\n",
    "        else:\n",
    "            # Day is NOT in any spell window\n",
    "            if is_predicted:\n",
    "                FP += 1  # False alarm\n",
    "            else:\n",
    "                TN += 1  # Correct non-prediction\n",
    "    \n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else np.nan\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 and not np.isnan(precision) and not np.isnan(recall) else np.nan\n",
    "    far = FP / (TP + FP) if (TP + FP) > 0 else np.nan\n",
    "    \n",
    "    return TP, FN, FP, TN, precision, recall, f1, far\n",
    "\n",
    "# Load obs once\n",
    "obs_df_full = pd.read_excel(obs_excel)\n",
    "obs_df_full['Date'] = pd.to_datetime(obs_df_full['Date Time']).dt.normalize()\n",
    "\n",
    "for season, models in model_dirs.items():\n",
    "    print(f\"\\n===== {season} =====\")\n",
    "    sm, em = season_months(season)\n",
    "    allowed_months = list(range(sm, em + 1))\n",
    "    \n",
    "    # Combined output dir for this season\n",
    "    out_dir = combined_root / season\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Saving to: {out_dir}\")\n",
    "\n",
    "    all_dates = build_all_dates(years, sm, em)\n",
    "    \n",
    "    results_rows = []\n",
    "    for model_name, dir_path in models.items():\n",
    "        if not dir_path.exists():\n",
    "            print(f\"  ❌ {model_name}: not found\")\n",
    "            continue\n",
    "        \n",
    "        # Load flood spells for this model/season\n",
    "        spells_df = load_flood_spells(dir_path, years)\n",
    "        if spells_df.empty:\n",
    "            print(f\"  ❌ {model_name}: no flood spell data found\")\n",
    "            continue\n",
    "        \n",
    "        spell_windows, day_to_spells = build_spell_windows(spells_df)\n",
    "        print(f\"  → Loaded {len(spell_windows)} flood spells for spell-based evaluation\")\n",
    "        \n",
    "        # Extract predictions\n",
    "        pred_by_lead = extract_pred_from_sumf(dir_path, years, allowed_months, all_dates)\n",
    "        if not pred_by_lead:\n",
    "            print(f\"  ❌ {model_name}: no predictions extracted\")\n",
    "            continue\n",
    "        \n",
    "        # Union-of-leads\n",
    "        predicted_union = set()\n",
    "        for s in pred_by_lead.values():\n",
    "            predicted_union |= s\n",
    "\n",
    "        # Compute metrics using SPELL-BASED LOGIC\n",
    "        TP, FN, FP, TN, P, R, F1, FAR = compute_confusion_spell_based(\n",
    "            all_dates, spell_windows, day_to_spells, predicted_union\n",
    "        )\n",
    "\n",
    "        # Save overall (union)\n",
    "        overall_csv = out_dir / f\"{model_name.replace(' ', '_')}_overall.csv\"\n",
    "        pd.DataFrame([{\n",
    "            'Model': model_name, \n",
    "            'TP': TP, 'FN': FN, 'FP': FP, 'TN': TN, \n",
    "            'Precision': P, 'Recall': R, 'F1 Score': F1, 'FAR': FAR\n",
    "        }]).to_csv(overall_csv, index=False)\n",
    "        \n",
    "        # Save per-lead (also using spell-based logic)\n",
    "        per_lead_rows = []\n",
    "        for lead, preds in pred_by_lead.items():\n",
    "            t, f, fp, tn, p, r, f1, far = compute_confusion_spell_based(\n",
    "                all_dates, spell_windows, day_to_spells, preds\n",
    "            )\n",
    "            per_lead_rows.append({\n",
    "                'Lead Day': lead, \n",
    "                'TP': t, 'FN': f, 'FP': fp, 'TN': tn, \n",
    "                'Precision': p, 'Recall': r, 'F1 Score': f1, 'FAR': far\n",
    "            })\n",
    "        per_lead_csv = out_dir / f\"{model_name.replace(' ', '_')}_per_lead.csv\"\n",
    "        pd.DataFrame(per_lead_rows).sort_values('Lead Day').to_csv(per_lead_csv, index=False)\n",
    "        \n",
    "        print(f\"  ✓ {model_name}: saved overall & per-lead (spell-based metrics)\")\n",
    "        results_rows.append({\n",
    "            'Model': model_name, \n",
    "            'TP': TP, 'FN': FN, 'FP': FP, 'TN': TN, \n",
    "            'Precision': round(P, 3), 'Recall': round(R, 3), \n",
    "            'F1': round(F1, 3), 'FAR': round(FAR, 3)\n",
    "        })\n",
    "\n",
    "    # Season comparison\n",
    "    if results_rows:\n",
    "        comp = pd.DataFrame(results_rows)\n",
    "        comp_csv = out_dir / \"overall_comparison.csv\"\n",
    "        comp.to_csv(comp_csv, index=False)\n",
    "        print(f\"\\nSeason comparison (SPELL-BASED METRICS):\")\n",
    "        print(f\"Hit window = [Spell_Start-2d, Spell_End]: predictions here = TP\")\n",
    "        print(f\"Actual flood = [Spell_Start, Spell_End]: NO predictions here = FN\")\n",
    "        print(f\"Early warning = [Spell_Start-2d, Spell_Start-1]: NOT counted as miss if empty\")\n",
    "        print(comp.to_string(index=False))\n",
    "        print(f\"Saved to: {comp_csv}\")\n",
    "\n",
    "print(f\"\\n✓✓✓ All outputs in combined folder: {combined_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb2ae95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
